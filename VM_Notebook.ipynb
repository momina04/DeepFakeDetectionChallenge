{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch-lightning\n",
    "!pip install opencv-python\n",
    "!pip install git+https://github.com/ryanwongsa/kaggle-api.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    import wandb\n",
    "    HAS_WANDB = True\n",
    "except ImportError:\n",
    "    HAS_WANDB = False\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from models.efficientnet.net import Net\n",
    "from feature_detectors.face_detectors.facenet.face_detect import MTCNN\n",
    "from dataloader.video_dataset import VideoDataset\n",
    "\n",
    "from lightning.helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningSystem(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(LightningSystem, self).__init__()\n",
    "        \n",
    "        # -------------PARAMETERS--------------\n",
    "        wandb_project_name = \"test-project\"\n",
    "        \n",
    "        # model parameters\n",
    "        network_name = 'efficientnet-b0'\n",
    "        \n",
    "        # face detection parameters\n",
    "        face_img_size = 64\n",
    "        face_keep_all = False\n",
    "        face_thresholds = [0.6, 0.7, 0.7]\n",
    "        face_select_largest = True\n",
    "        face_margin = 10\n",
    "        \n",
    "        # dataloader parameters\n",
    "        self.bs = 8\n",
    "        self.num_workers = 0\n",
    "        self.num_frames = 5\n",
    "        \n",
    "        self.train_root_dir = \"/dltraining/datasets\"\n",
    "        self.train_metadata_file = \"/dltraining/datasets/train_metadata.json\"\n",
    "        self.isBalanced = True\n",
    "        \n",
    "        self.val_root_dir = \"/dltraining/datasets\"\n",
    "        self.val_metadata_file = \"/dltraining/datasets/valid_metadata.json\"\n",
    "        \n",
    "        self.test_root_dir = \"/dltraining/datasets/test_videos\"\n",
    "        \n",
    "        # training parameters\n",
    "        self.num_training_face_samples = 32\n",
    "        self.lr = 0.0003\n",
    "        # -------------PARAMETERS--------------       \n",
    "        \n",
    "        self.face_img_size = face_img_size\n",
    "        self.model = Net(network_name)\n",
    "        \n",
    "        device = torch.device('cuda' if self.on_gpu else 'cpu')\n",
    "        \n",
    "        self.fd_model = MTCNN(image_size=self.face_img_size, keep_all=face_keep_all, device=device,thresholds=face_thresholds, select_largest=face_select_largest, margin=face_margin)\n",
    "        self.fd_model.eval()\n",
    "        \n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.log_loss = nn.BCELoss()\n",
    "\n",
    "        self.transform = transforms.Compose([transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "        \n",
    "        if HAS_WANDB:\n",
    "            wandb.init(project=wandb_project_name, sync_tensorboard=True)\n",
    "            wandb.watch(self.model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        source_filenames, videos, labels, video_original_filenames = batch\n",
    "        \n",
    "        videos_faces, videos_labels = detect_faces_for_videos(self.fd_model, self.face_img_size, videos, labels)\n",
    "        \n",
    "        videos_faces, videos_labels = get_samples(videos_faces, videos_labels,self.num_training_face_samples)\n",
    "        videos_faces = transform_batch(videos_faces, self.transform)\n",
    "\n",
    "        predicted = self.forward(videos_faces).squeeze()\n",
    "        loss = self.criterion(predicted, videos_labels)\n",
    "\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        source_filenames, videos, labels, video_original_filenames = batch\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        total_logloss = 0.0\n",
    "        for video, label in zip(videos, labels):\n",
    "            video_label = label[0]\n",
    "\n",
    "            faces, face_labels = detect_video_faces(self.fd_model, self.face_img_size, video, label)\n",
    "            if len(faces)>0:\n",
    "                faces = transform_batch(faces, self.transform)\n",
    "                predictions = self.forward(faces).squeeze()\n",
    "                loss = self.criterion(predictions, face_labels)\n",
    "                logloss = self.log_loss(predictions.mean(), video_label)\n",
    "            else:\n",
    "                logloss = 0.7\n",
    "                loss = 0.7\n",
    "\n",
    "            total_logloss += logloss \n",
    "            total_loss += loss\n",
    "            \n",
    "        avg_loss = total_loss / len(videos)\n",
    "        avg_logloss = total_logloss / len(videos)\n",
    "        return {'val_loss': avg_loss, 'logloss':avg_logloss}\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        logloss = torch.tensor([x['logloss'] for x in outputs]).mean()\n",
    "        return {'val_loss': loss, 'val_logloss': logloss, 'log': {'val_loss': loss, 'val_logloss':logloss}, 'progress_bar': {'val_loss': loss, 'val_logloss':logloss}}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        source_filenames, videos = batch\n",
    "\n",
    "        list_submission = []\n",
    "        for source_filename, video in zip(source_filenames, videos):\n",
    "            faces, _ = detect_video_faces(self.df_model, self.face_img_size, video)\n",
    "            if len(faces)>0:\n",
    "                faces = transform_batch(faces, self.transform)\n",
    "                predictions = self.forward(faces).squeeze()\n",
    "                if self.on_gpu:\n",
    "                    predictions = float(predictions.mean().cpu().detach().numpy())\n",
    "                else:\n",
    "                    predictions = float(predictions.mean().detach().numpy())\n",
    "            else:\n",
    "                predictions = 0.5\n",
    "\n",
    "            dict_solution = {\n",
    "              \"filename\":source_filename,\n",
    "              \"label\": predictions\n",
    "            }\n",
    "            list_submission.append(dict_solution)\n",
    "        return {'submission_batch': list_submission}\n",
    "\n",
    "    def test_end(self, outputs):\n",
    "        list_submission = []\n",
    "\n",
    "        for output in outputs:\n",
    "            list_submission += output[\"submission_batch\"]\n",
    "        df = pd.DataFrame(list_submission)\n",
    "        df.to_csv(\"submission.csv\", index=False)\n",
    "        return {}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        train_dataset = VideoDataset(self.train_root_dir, self.train_metadata_file, isBalanced=self.isBalanced, num_frames=self.num_frames)\n",
    "        train_dataloader = DataLoader(train_dataset,\n",
    "                batch_size= self.bs,\n",
    "                shuffle= False, \n",
    "                num_workers= self.num_workers, \n",
    "                collate_fn= train_dataset.collate_fn,\n",
    "                pin_memory= True, \n",
    "                drop_last = True,\n",
    "                worker_init_fn=train_dataset.init_workers_fn\n",
    "            )\n",
    "        return train_dataloader\n",
    "\n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        val_dataset = VideoDataset(self.val_root_dir, self.val_metadata_file, num_frames=self.num_frames)\n",
    "        val_dataloader = DataLoader(val_dataset,\n",
    "                batch_size= self.bs,\n",
    "                shuffle= False, \n",
    "                num_workers= self.num_workers, \n",
    "                collate_fn= val_dataset.collate_fn,\n",
    "                pin_memory= True, \n",
    "                drop_last = False,\n",
    "                worker_init_fn=val_dataset.init_workers_fn\n",
    "            )\n",
    "        return val_dataloader\n",
    "    \n",
    "    @pl.data_loader\n",
    "    def test_dataloader(self):\n",
    "        dataset = VideoDataset(self.test_root_dir, None, num_frames=self.num_frames)\n",
    "        dataloader = DataLoader(dataset,\n",
    "                batch_size= self.bs,\n",
    "                shuffle= False, \n",
    "                num_workers= self.num_workers, \n",
    "                collate_fn= dataset.collate_fn,\n",
    "                pin_memory= True, \n",
    "                drop_last = False,\n",
    "                worker_init_fn=dataset.init_workers_fn\n",
    "            )\n",
    "        return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "model = LightningSystem()\n",
    "\n",
    "# # DEFAULTS used by the Trainer\n",
    "# checkpoint_callback = ModelCheckpoint(\n",
    "#     filepath=os.getcwd(),\n",
    "#     save_best_only=False,\n",
    "#     verbose=True,\n",
    "#     monitor='val_loss',\n",
    "#     mode='min',\n",
    "#     prefix=''\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(nb_sanity_val_steps=0, gpus=1, max_nb_epochs=10, train_percent_check=1.0, val_percent_check=1.0, checkpoint_callback=checkpoint_callback)\n",
    "# trainer.fit(model)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(gpus=1, nb_sanity_val_steps=0, fast_dev_run=True)\n",
    "trainer.fit(model)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(gpus=1, nb_sanity_val_steps=0,overfit_pct=0.01)\n",
    "trainer.fit(model)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
